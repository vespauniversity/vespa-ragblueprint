{"loc": "/media/albert/sda/Code/rag/vespa-ragblueprint/dataset/Vespa Enterprise 2-pager (Nov 2025).pdf", "text": "Maximize Performanceand Scale without LimitsAccelerate innovation, improve reliability, and lower TCO for production-scaleVespa deployments while maintaining full control over security and data privacy.Benefits of EnterpriseTrusted ByManagedInfrastructureContinuousDeploymentVESPA ENTERPRISE PLANBuilt-In Security24/7 SupportEarly AccessVespa Enterprise removes the burden of runningyour own infrastructure so your team can stayfocused on building, shipping, and improvingapplications. Optimize frequently without disruption, resizeinfrastructure as schemas and workloads evolve,and use committed spend options to reducecost even further. The result is faster iteration, lower operationalrisk, and a production-ready environment forlarge-scale RAG, search, and recommendationworkloads.Onyx.app reduced infrastructure costs by 24.5% using Vespa's automated resourceoptimization features. The platform analyzed their workload patterns and identified CPUoverprovisioning, recommending a cluster reconfiguration from 120 nodes to 60 nodes,with zero downtime, requiring only a three-line configuration change.Yuhong Sun, Cofounder Onyx.appvespa.aifernando@vespa.ai\f696M monthlyactive users“Vespa let uscollapse 3retrieval tiers into1 engine we canactually reasonabout. Everyplaylist now feelslike it washandcrafted foryou because,under the hood,it basically was.”+For Onyx.app, Vespa’s automated CI/CD,seamless node migrations, and fault-tolerantoperations turned continuous optimization intoa safe, push-button experience. Tasks likeprovisioning, upgrades, node replacements, andload-balancing disappeared overnight. With Vespa’s real-time monitoring andintelligent resource suggestions, Onyx instantlyidentified opportunities to right-size theircluster and cut costs by 24.5%, without anydowntime. The result: a faster, leaner, more reliable AIplatform and a team freed to innovate at fullspeed.after switching fromElasticsearch to Vespa2.5x fasterDaniel Doro,Director of SearchEngineering,Spotify150+ online, real-timeapplications10B-documentglobal RAG index 800 queries persecond (QPS) fromacross the globe10X traffic increasewith zero impact onlatency100M+ songs and7M+ podcasts100B+ multi-modaldocuments120M monthly users(2M+ daily)32% YoY cataloggrowth withoutincreasing infra costsAlways-on, real-timepersonalization.Internet-scale RAG atconsumer search speed.Recommendations atglobal scale.vespa.aifernando@vespa.aiPrecision at Scale", "title": "Vespa Enterprise 2-pager (Nov 2025)", "timestamp": "2026-02-01T20:55:27.304828"}
{"loc": "/media/albert/sda/Code/rag/vespa-ragblueprint/dataset/Deploying RAG at Scale_ Key Questions for Vendors _ Vespa Blog.pdf", "text": "1/2/26, 1:52 PM\n\n{ }\n\nDeploying RAG at Scale: Key Questions for Vendors | Vespa Blog\n\nVespa Blog\nWe Make AI Work\n\nSUBSCRIBE\n\nTim Young\n\nChief Marketing Officer\n\n28 Oct 2024\n\nDeploying RAG at Scale: Key Questions for\nVendors\n\nRetrieval-augmented generation (RAG) has emerged as a vital technology for organizations embracing generative AI. By connecting large\n\nlanguage models (LLMs) to corporate data in a controlled and secure manner, RAG enables AI to be deployed in specific business use cases,\n\nsuch as enhancing customer service through conversational AI. For those new to RAG, I recommend this BARC research note: Why and How\n\nRetrieval-Augmented Generation Improves GenAI Outcomes, available for free here.\n\nIn its recent Hype Cycle for Generative AI, Gartner identifies RAG as an early-stage technology driving innovation. However, it is\n\napproaching a peak of inflated expectations as ambitions for RAG outpace the practicalities of deploying it at scale. Vendor exuberance\n\nlikely raises the bar on expectations—hype around RAG and generative AI is at a fever pitch!\n\nOur discussions with large enterprises reveal that while generative AI pilots are proving value, scaling these solutions across the enterprise\n\nis a concern. Managers ask:\n\nHow can we scale from concept to enterprise-wide deployment?\n\nGiven the intensive processing demands of generative AI, how can I control costs?\n\nHow can I ensure compliance with data privacy and security regulations?\n\nHow can I integrate all relevant data sources beyond just vector databases?\n\nHow can I stay current with emerging technologies and best practices?\n\nhttps://blog.vespa.ai/deploying-rag-at-scale/\n\n1/4\n\n\f1/2/26, 1:52 PM\n\nDeploying RAG at Scale: Key Questions for Vendors | Vespa Blog\n\nBefore answering these questions, let’s introduce Vespa:\n\nVespa is a robust platform for developing real-time, search-based AI applications. Its large-scale distributed architecture enables efficient\n\ndata processing, inference, and logic management, making it ideal for applications handling vast datasets and high volumes of concurrent\n\nqueries.\n\nFrom Concept to Enterprise Deployment\n\nProving the value of RAG in the lab is one thing, but scaling it across an entire enterprise introduces numerous challenges. These include\n\nintegrating with existing data sources, ensuring strict data privacy and security, delivering required performance, and managing this\n\ncomplex large-scale run-time environment. Scalability is also a significant concern, as AI models must handle vast amounts of growing\n\ndata and increasingly diverse use cases while maintaining high performance and reliability.\n\nVespa has been wrestling with these challenges since 2011—long before AI hit the mainstream. Originally developed to address Yahoo’s\n\nlarge-scale requirements, Vespa runs 150 applications integral to the company’s operations. These applications deliver personalized\n\ncontent across Yahoo in real-time and manage targeted advertisements within one of the world’s largest ad exchanges. Collectively, these\n\napplications serve an impressive user base of nearly one billion individuals, processing 800,000 queries per second.\n\nVespa offers two essential components for enterprise RAG deployment:\n\na comprehensive platform for developing generative AI applications\n\na scalable deployment architecture to address the demands of large enterprises.\n\nThe Platform Approach\n\nVespa is a fully integrated platform that offers all the essential components needed to build robust AI applications. It includes a versatile\n\nvector database, hybrid search capabilities, RAG, natural language processing (NLP), machine learning, and LLM support. The platform\n\nconnects easily with existing operational systems and databases through APIs and SDKs, enabling AI applications to support your specific\n\nrequirements. This allows organizations to integrate existing data infrastructure easily.\n\nVespa’s hybrid search capabilities enhance the accuracy of generative AI by combining various data types, including vectors, text, and both\n\nstructured and unstructured information. Machine learning algorithms score and rank results to align with user intent, delivering precise\n\nand relevant answers. A key feature of the platform is its advanced natural language processing, which enables efficient semantic search.\n\nBy understanding the meaning behind user queries rather than just matching keywords, Vespa supports vector search with embeddings\n\nand integrates custom or pre-trained machine learning models for more precise content retrieval.\n\nVisual search is a hot topic, and Vespa offers intelligent document retrieval that combines images and text to enable detailed contextual\n\nsearches. This creates a visually intuitive search experience that feels more natural and human-like.\n\nAn Execution Environment for Large Scale Enterprise Deployment\n\nVespa Cloud streamlines large-scale deployment, delivering high performance but simplifying performance management to ensure a\n\nseamless user experience. Applications running on Vespa dynamically adjust to fluctuating workloads, optimizing performance and cost—\n\neliminating over-provisioning to keep costs in check and users happy.\n\nDesigned for high performance at scale, Vespa’s distributed architecture ensures instant query processing and advanced data\n\nmanagement. It offers low-latency query execution, real-time data updates, and sophisticated ranking algorithms, enabling enterprises to\n\nefficiently process and utilize data across their operations without sacrificing speed or accuracy.\n\nThe platform’s robust, always-on architecture guarantees uninterrupted service. By distributing data, queries, and machine learning\n\nmodels across multiple nodes, Vespa achieves high availability and fault tolerance, ensuring continuous operation even under demanding\n\nconditions.\n\nSecurity and compliance are core elements of Vespa’s design. With computation performed close to the data and distributed across nodes,\n\nthe platform reduces network bandwidth costs and minimizes latency. It adheres to data residency and security policies, with encryption\n\nat rest and secure, authenticated internal communications between nodes. This comprehensive approach provides a secure and governed\n\nenvironment for deploying AI applications at scale.\n\nhttps://blog.vespa.ai/deploying-rag-at-scale/\n\n2/4\n\n\f1/2/26, 1:52 PM\n\nDeploying RAG at Scale: Key Questions for Vendors | Vespa Blog\n\nFuture Prooﬁng\n\nRAG deployments naturally evolve as experience and confidence grow and use case requirements become more sophisticated. What begins\n\nas a basic Q&A system for tasks like customer service chatbots can scale into dynamic, live knowledge bases that are rapidly consulted\n\nhundreds or even thousands of times, drawing on conclusions reached by the AI. Vespa supports this stepwise development, allowing for\n\ncontrolled, incremental rollouts that adapt to evolving needs.\n\nFuture-proofing also involves adopting the latest technologies and best practices. For example, Vespa enables visual search capabilities in\n\neCommerce, where searches are driven by images, and supports standards like ColPali for large-scale PDF search. With Vespa Cloud, our\n\nengineers continually integrate emerging RAG best practices, ensuring your enterprise stays ahead. We incorporate this best practice so\n\nthat you can focus on your AI applications.\n\nSummary\n\nRAG is crucial for businesses adopting generative AI. However, scaling it across an enterprise presents challenges, including integration,\n\ndata privacy, infrastructure management, and performance at scale. Vespa addresses these challenges with a comprehensive platform and\n\nscalable deployment architecture. Proven by Yahoo’s large-scale needs, Vespa Cloud supports AI applications with real-time, low-latency\n\nquery processing, hybrid search, and advanced data processing.\n\nRead more\n\nVisual RAG over PDFs with Vespa - A demo application in Python\n\nThis is a technical blog post on developing an end-to-end Visual RAG application powered by Vespa. It has link to a live demo application, and will walk you\n\nthrough why...\n\nBarc Research report\n\nWhy and How Retrieval-Augmented Generation Improves GenAI Outcomes\n\nhttps://blog.vespa.ai/deploying-rag-at-scale/\n\n3/4\n\n\f1/2/26, 1:52 PM\n\nDeploying RAG at Scale: Key Questions for Vendors | Vespa Blog\n\nFree Trial\n\nDeploy your application for free. Get started now to get $300 in free credits. No credit card required!\n\n« Announcing support for global significance models\n\nVespa.ai: The “Sleeping Giant” Powering Next-Gen Search and Recommendations »\n\nShare\n\n      \n\nCopyright © 2025 Vespa Blog - Subscribe to updates - Vespa Blog RSS feed -\nCookie Preferences\n\nMediumish Jekyll Theme by WowThemes.net\n\nhttps://blog.vespa.ai/deploying-rag-at-scale/\n\n4/4\n\n", "title": "Deploying RAG at Scale_ Key Questions for Vendors _ Vespa Blog", "timestamp": "2026-02-01T20:55:31.415083"}
{"loc": "/media/albert/sda/Code/rag/vespa-ragblueprint/dataset/Vespa.ai Proof of Concept (POC) Doc.docx", "text": "Vespa.ai Proof of Concept (POC) Doc\n\nThis Proof of Concept (POC) Doc includes expectations, deliverables, and timelines (from both customer and Vespa.ai) to ensure proper alignment as we look towards running a successful POC that validates the expected value of Vespa.ai, and guides the customer from concept to production application.\n\nThese steps include technical alignment, validating timeline, developing business case, creating evaluation checklist, and onboarding prep.\n\n## Technical Alignment\n\n### Use Case:\n\nBelow are a set of example use cases to test the Vespa.ai Retrieval Platform. This should be replaced with the Customer’s scoped use cases.\n\nSearch: Index millions of SKUs, accept updates in seconds, and combine keyword, vector, and structured filters in one query.\n\nRecommender System: Store user, item, and contextual signals together; run candidate-generation and learned-to-rank models in-place.\n\nRetrieval-Augmented Generation (RAG): Serve fresh, citation-ready chunks to an LLM from millions of docs. Gauge accuracy, latency, and infra spend.\n\n### Technical Work:\n\nThe basics.\n\n**Goal**: Get an initial application up and running to form a basis for review and further work.\n\n1. Sign up for Vespa Cloud (includes free credits, and more can be requested)\n   1. https://aws.amazon.com/marketplace/pp/prodview-5pkxkencasnoo\n2. Complete the Getting Started Guide\n3. Attend live training (scheduled by sales)\n4. Schedule working session(s) to assist in any steps\n\n## Success Criteria\n\nThe goal of the POC is to deliver on the outcomes that the customer is aiming to achieve with a retrieval platform. Understanding what success looks like for the customer, with both short-term objectives and long-term goals in mind, will help the Vespa.ai team to make the proper recommendations on solution design, training + workshops, and infra sizing.\n\nThe POC Schedule, Business Case, and Evaluation Checklist are designed to help facilitate the evaluation of Vespa.ai as a viable solution for the desired customer outcomes.\n\n### POC Schedule:\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n| **Date (\\*subject to change)** | **Deliverable/Action** | **Complete** | **Notes** |\n| TBD\\* | Mutual NDA Completed |  |  |\n| TBD\\* | Align on POC Doc |  |  |\n| TBD\\* | POC Agreement Completed |  |  |\n| Week 1 | POC Kickoff (Setup + Config) |  |  |\n| Week 2 | Performance Optimization |  |  |\n| Week 3 | Cost Optimization |  |  |\n| Week 4 | Review Performance against Evaluation Checklist |  |  |\n| Week 5 | Prepare for Production |  |  |\n| Week 6 | Align on Next Steps |  |  |\n| Week 7\\* | Finalize Licensing + Support Tier |  |  |\n| Week 8\\* | Onboarding Kickoff with Customer Success |  |  |\n\n### Business Case & ROI:\n\nSupplemental reading:\n\nhttps://content.vespa.ai/barc-rag\n\nhttps://content.vespa.ai/gigaom-report-2025\n\nSee Vespa.ai TCO + ROI Analysis spreadsheet:\n\nLink\n\n### Evaluation Checklist:\n\nSupplemental reading:\n\nhttps://vespa.ai/rag-managers-guide\n\nhttps://content.vespa.ai/gigaom-report-2025\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n| **Evaluation Checklist** | **Priority** | **Pass/Fail** | **Evaluation Notes** |\n| **Overview** |  |  |  |\n| Vector similarity search |  |  |  |\n| Structured data search |  |  |  |\n| Full-text search |  |  |  |\n| Combine multiple search criteria and modes in one query |  |  |  |\n| Grouping/aggregation/faceting |  |  |  |\n| Rank by any signals, using any function or ML model |  |  |  |\n| Realtime writes |  |  |  |\n| Seamless scaling |  |  |  |\n| Application level deployments and management |  |  |  |\n| Proven in production at scale |  |  |  |\n| Optimized for timeseries analytics |  |  |  |\n| Open source |  |  |  |\n| Managed option |  |  |  |\n| Embeddable |  |  |  |\n| **Vector features** |  |  |  |\n| Multiple vector fields per document |  |  |  |\n| Multiple vector values per document field |  |  |  |\n| Unlimited vector dimensions |  |  |  |\n| Efficient vector field writes |  |  |  |\n| WAND text retrieval |  |  |  |\n| Fuzzy retrieval |  |  |  |\n| Exact vector retrieval |  |  |  |\n| GEO retrieval |  |  |  |\n| Search multiple schemas/sources in a query |  |  |  |\n| **Inference, computation and ranking features** |  |  |  |\n| General tensor support |  |  |  |\n| Sparse vector/tensor dimensions |  |  |  |\n| General computation/inference over features at ranking time |  |  |  |\n| GEO signals |  |  |  |\n| Text matching signals |  |  |  |\n| Text proximity ranking signals |  |  |  |\n| Distributed second-phase re-ranking |  |  |  |\n| Global-phase reranking |  |  |  |\n| Return any inferred scalars/tensors with results |  |  |  |\n| GBDT ML model evaluation |  |  |  |\n| Onnx ML model evaluation |  |  |  |\n| Multiple rank/inference profiles per schema |  |  |  |\n| **Performance and efficiency features** |  |  |  |\n| Cost effective and complete vector retrieval of personal data |  |  |  |\n| Configurable vector precision |  |  |  |\n| Multiple threads per query per node |  |  |  |\n| Paged fields |  |  |  |\n| Choose which fields to index |  |  |  |\n| Choose which fields to return in a response |  |  |  |\n| Parent-child denormalized fields |  |  |  |\n| **Application development and management features** |  |  |  |\n| Vector embedding performed inside the engine |  |  |  |\n| Query/result/write components as part of the application |  |  |  |\n| Declarative index mapping |  |  |  |\n| Automatic data garbage collection |  |  |  |\n| Collection fields |  |  |  |\n| Linguistics processing |  |  |  |\n| Query profiles |  |  |  |\n| Write data without a schema |  |  |  |\n| Writes are validated against schema |  |  |  |\n| Safely change schemas while online |  |  |  |\n| Multiple schemas per application |  |  |  |\n| Multiple clusters per application |  |  |  |\n| Scaling: Multiple replicas with automatic load balancing |  |  |  |\n| Scaling: Multiple shards with scatter-gather |  |  |  |\n| Scaling: Make any change to topology resources while online |  |  |  |\n| Scaling: Fully automatic shard placement and configuration |  |  |  |\n| Cross-cluster replication |  |  |  |\n| **Other features** |  |  |  |\n| Text snippeting |  |  |  |\n| Predicate fields |  |  |  |\n| Rich features for timeseries analytics |  |  |  |\n| Generative text integration |  |  |  |\n| **Managed service** |  |  |  |\n| Safe global continuous deployment of applications |  |  |  |\n| Automatic safe platform upgrades |  |  |  |\n| Control the resources of the application |  |  |  |\n| Autoscaling |  |  |  |\n| Multi-region/cloud deployments |  |  |  |\n| AWS zones |  |  |  |\n| GCP zones |  |  |  |\n| Azure zones |  |  |  |\n| Managed in customer account/project/subscription (BYOC) |  |  |  |", "title": "Vespa.ai Proof of Concept (POC) Doc", "timestamp": "2026-02-01T20:55:42.204868"}

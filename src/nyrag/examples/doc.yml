name: doc
mode: docs
start_loc: ./dataset
deploy_mode: cloud  # "local" for Docker or "cloud" for Vespa Cloud
vespa_app_path: ./vespa_cloud

# Vespa Cloud configuration - set tenant at top level
cloud_tenant: your-tenant  # required for cloud mode (env/CLI still override)
vespa_cloud:
  # enpoint (TOKEN version not the MTLS)
  endpoint: https://your-vespa-cloud-endpoint-here.vespa-app.cloud
  # token: YOUR_TOKEN_HERE  # Or set VESPA_CLOUD_SECRET_TOKEN env var
  token: your-vespa-cloud-token-here

exclude:
  - "*.html"

# Optional: Document processing specific parameters
doc_params:
  recursive: true               # Process subdirectories recursively (default: true)
  include_hidden: false         # Include hidden files (default: false)
  follow_symlinks: false        # Follow symbolic links (default: false)
  max_file_size_mb: 100         # Maximum file size in MB (optional)
  file_extensions:              # Only process these file types (optional)
    - .pdf
    - .docx
    - .txt
    - .md

# Optional: General parameters for downstream RAG processing
# Note: Embeddings are computed by Vespa's HuggingFace embedder (nomic-ai-modernbert-embed-base)
# Distance metric is fixed to hamming (binary vectors with pack_bits)
rag_params:
  embedding_model: nomic-ai/modernbert-embed-base  # Model used by Vespa embedder
  embedding_dim: 96               # Packed int8 dimension (768 floats -> 96 int8)
  chunk_size: 512                 # Chunk size for text splitting (default: 1024)
  chunk_overlap: 0                # Vespa's built-in chunking doesn't use overlap
  max_tokens: 8192

# Optional: LLM configuration (works with any OpenAI-compatible API)
llm_config:
  # For OpenRouter (default):
  base_url: https://openrouter.ai/api/v1
  model: openai/gpt-4o-mini
  api_key: your-llm-api-key-here

name: webrag
mode: web
start_loc: https://vespa.ai/
deploy_mode: local  # "local" for Docker or "cloud" for Vespa Cloud
cloud_tenant: your-tenant  # required for cloud mode (env/CLI still override)
exclude:
  - https://vespa.ai/pricing
  - https://vespa.ai/sales
  - https://status.vespa.ai/*

# Optional: Web crawling specific parameters
crawl_params:
  respect_robots_txt: true     # Respect robots.txt rules (default: true)
  aggressive_crawl: false       # Enable aggressive crawling (higher speed, more requests) (default: false)
  follow_subdomains: true       # Follow subdomains of the start URL (default: true)
  strict_mode: false            # Only crawl URLs matching the start URL pattern (default: false)
  user_agent_type: chrome       # User agent type: chrome, firefox, safari, mobile, bot (default: chrome)
  # custom_user_agent: "..."    # Custom user agent string (optional, overrides user_agent_type)
  allowed_domains:            # Explicitly allowed domains (optional, auto-detected from start_loc)
    - vespa.ai
    - docs.vespa.ai

# Optional: General parameters for downstream RAG processing
# Note: Embeddings are computed by Vespa's HuggingFace embedder (nomic-ai-modernbert-embed-base)
# Distance metric is fixed to hamming (binary vectors with pack_bits)
rag_params:
  embedding_model: nomic-ai/modernbert-embed-base  # Model used by Vespa embedder
  embedding_dim: 96               # Packed int8 dimension (768 floats -> 96 int8)
  chunk_size: 1024                # Chunk size for text splitting (default: 1024)
  chunk_overlap: 0                # Vespa's built-in chunking doesn't use overlap
  max_tokens: 8192

# Optional: LLM configuration (works with any OpenAI-compatible API)
llm_config:
  # For OpenRouter (default):
  base_url: https://openrouter.ai/api/v1
  model: anthropic/claude-3.5-sonnet
  api_key: your-openrouter-key
  
  # For Ollama (local):
  # base_url: http://localhost:11434/v1
  # model: llama3.2
  # api_key: dummy
  
  # For LM Studio (local):
  # base_url: http://localhost:1234/v1
  # model: openai/gpt-oss-20b
  # api_key: dummy
  
  # For vLLM (local or remote):
  # base_url: http://localhost:8000/v1
  # model: meta-llama/Llama-3.2-3B-Instruct
  # api_key: dummy
